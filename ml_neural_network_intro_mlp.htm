<html>
<head>
	<title>線上教材：音樂資訊分析</title>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8" />
	<link rel=stylesheet type="text/css" href="myCss.css">
	<base target="_blank">
	<script type="text/javascript" src="shCore.js"></script>
	<script type="text/javascript" src="shBrushPython.js"></script>
	<link href="shCore.css" rel="stylesheet" type="text/css" />
	<link href="shThemeDefault.css" rel="stylesheet" type="text/css" />
	<script type="text/javascript">
		SyntaxHighlighter.all();
	</script>
</head>

<body bgcolor="#ccccff">

<blockquote>

<p>
大家在國高中生物課程中學過生物的神經，有樹突作為輸入端，而當輸入的強度超過閾值時，就會啟動輸出，將訊號傳遞到下一個神經元；而類神經網路，則是比照生物學的原理，以人工的方式來模擬神經元的行為。在最簡單的，只包含一個輸入端和一個神經元的情況中，我們可以用數學將其表示成 y&#770; = f( x * w + b)，其中的 x 是輸入值，w 和 b 分別是我們需要訓練來代表神經元行為的權重(weight)和偏差(bias)值，f 稱為激勵函數(activation function)，用於模擬生物上的神經元，在超過強度閾值時才會有輸出的這種行為，y&#770; 則為神經元的輸出值。若考慮到生物的神經元中，樹突會有多個輸入端的情況，則前面的式子會變成 y&#770; = f(<b>x</b><sup>T</sup><b>w</b> + b)，此時的 <b>x</b> 和 <b>w</b> 都各自是一個 column vector，而經過運算的 y&#770; 還是單一數值。進一步再考慮輸出端也有多個的情況，則式子會再進一步變成 <b>y&#770;</b> = f(<b>x</b><sup>T</sup>W + <b>b</b>)，此時的輸入 <b>x</b>、偏差 <b>b</b> 和輸出 <b>y&#770;</b> 都是向量，而 W 是矩陣。
</p>

<p>
把這樣的神經元堆疊多層，就是大家可能早已熟悉到變成基本常識的多層感知器(Multilayer perceptron, MLP)，並且由於每一層的每個輸出都是完全連接到下一層的每個輸入，所以這樣的每一層叫做全連接層(fully connected layer)。如果有 n 層的話，這個網路寫成算式會是這樣： <b>h<sub>1</sub></b> = f(<b>x</b><sup>T</sup>W<sub>1</sub> + <b>b<sub>1</sub></b>), <b>h<sub>2</sub></b> = f(<b>h<sub>1</sub></b><sup>T</sup>W<sub>2</sub> + <b>b<sub>2</sub></b>), ..., <b>y&#770;</b> = f(<b>h<sub>n-1</sub></b><sup>T</sup>W<sub>n</sub> + <b>b<sub>n</sub></b>)。假設你已經知道了一路上的每個 W 和 <b>b</b>，則給定輸入 <b>x</b> 要計算 <b>y&#770;</b> 相當簡單，就是高中學過的矩陣運算而已，此步驟通常稱為向前傳播(forward propagation, forward pass)。但是我們要怎麼訓練類神經網路，也就是說要怎樣更新每個 W 和 <b>b</b> 呢？這需要先講到損失函數(loss function)，它的作用是評估網路算出來的 <b>y&#770;</b> 和標準答案的 <b>y</b> 之間差多少，要使用哪個或哪些損失函數是根據你的問題類型而定，例如回歸問題可以使用 mean squared error (MSE)，分類問題可以使用 cross entropy 等等。
</p>

<p>
Loss 計算出來以後，就是利用微分的方式求取梯度，以及利用各種最佳化的方法(optmizer)，例如 SGD (stochastic gradient descent)、Momentum、AdaGrad、RMSProp，以及 Adam 等，來更新每個 W 和 <b>b</b>；這一步驟由於是由算出來的 loss 倒回去進行，所以會稱為反向傳播(backpropagation)。如此反覆的向前以及反向傳播以後，通常就可以訓練出恰當的類神經網路。而除了怎樣訓練以外，你可能會想到另外一個問題是，疊到多深才算是深度學習？如果要爭論數字的話，可能每個人會有自己的看法，不一定要幾層才算是深；但就目前（2022 年）實際應用上會疊幾層網路的情況來說，影像處理常常是幾十層或一百層起跳，但音訊處理通常用個十多層就很了不起了，比較簡單的狀況甚至是三五層也很常見。
</p>

<p>若要使用 MLP 解決問題，有很多工具可以幫忙，常見的有像是 <a href="https://pytorch.org/">PyTorch</a>、<a href="https://www.tensorflow.org/?hl=zh-tw">TensorFlow</a>，以及 <a href="https://chainer.org/">Chainer</a> 等等，不過這些工具為了能讓你做更自由的調整，所以在真正的類神經網路本身和資料的事前處理，都有許多工作要自己做，因此以下還是先使用 scikit-learn 的 <a href="https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html">neural_network.MLPClassifier</a> 來示範 MLP 的使用，範例的資料集是 <a href="https://archive.ics.uci.edu/ml/datasets/wine">Wine Data Set</a>。</p>
<pre class="brush: py">
import numpy as np
from sklearn.datasets import load_wine
from sklearn.model_selection import train_test_split
from sklearn.neural_network import MLPClassifier

dataset = load_wine()
print('Data shapes:', dataset.data.shape, dataset.target.shape)

X_train, X_test, y_train, y_test = train_test_split(dataset.data, dataset.target)
print('Training data shapes:', X_train.shape, y_train.shape)
print('Test data shapes:', X_test.shape, y_test.shape)

model = MLPClassifier(hidden_layer_sizes=(256, 256))
model.fit(X_train, y_train)
pred = model.predict(X_test)
print('Accuracy: {:.2f}%'.format(100*np.mean(pred == y_test)))

hidden = np.maximum(X_test @ model.coefs_[0] + model.intercepts_[0], 0)
hidden = np.maximum(hidden @ model.coefs_[1] + model.intercepts_[1], 0)
pred = np.argmax(hidden @ model.coefs_[2] + model.intercepts_[2], axis=1)

print('Accuracy: {:.2f}%'.format(100*np.mean(pred == y_test)))
</pre>

<p>在上述範例中，我們只調整了隱藏層的數目，而其他如 learning rate 及 batch size 等參數則維持預設；範例中的隱藏層數目是一組有機會讓測試集準確率在 90% 上下（實際結果會隨著訓練與測試集的亂數切分而有差異）的參數，各位可以試著跟其他的參數一併調整看看。</p>

</blockquote>

</body></html>
